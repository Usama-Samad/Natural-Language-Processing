{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7728d83",
   "metadata": {},
   "source": [
    "## Usama Abdul Samad<br>\n",
    "## 311458<br>\n",
    "## Final Exercise Implementation Code<br>\n",
    "## Natural Language Processing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43325e11",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7801e4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from os import listdir\n",
    "import random\n",
    "from nltk.corpus import wordnet as wn\n",
    "import gensim\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import gensim.downloader\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category= RuntimeWarning)\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687cb32c",
   "metadata": {},
   "source": [
    "# Task No 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c364809",
   "metadata": {},
   "source": [
    "# Importing Semcor 3.0 Dataset Files one by one"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf71b11",
   "metadata": {},
   "source": [
    "Imported Brown 1 File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d9c29c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = []\n",
    "data_archive = 'semcor3.0\\\\brown1\\\\tagfiles'\n",
    "for files in os.listdir(data_archive):\n",
    "    with open(os.path.join(data_archive , files)) as fp:\n",
    "        soup = BeautifulSoup(fp)\n",
    "        tags = soup.find_all('s')\n",
    "        all_samples.extend(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76110246",
   "metadata": {},
   "source": [
    "Imported Brown 2 File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a931ec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_archive_2 = 'semcor3.0\\\\brown2\\\\tagfiles'\n",
    "for files_2 in os.listdir(data_archive_2):\n",
    "    with open(os.path.join(data_archive_2 , files_2)) as fp_2:\n",
    "        soup_2 = BeautifulSoup(fp_2)\n",
    "        tags_2 = soup_2.find_all('s')\n",
    "        all_samples.extend(tags_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330f37c9",
   "metadata": {},
   "source": [
    "Imported Brown V File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "5e69f99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_archive_3 = 'semcor3.0\\\\brownv\\\\tagfiles'\n",
    "for files_3 in os.listdir(data_archive_3):\n",
    "    with open(os.path.join(data_archive_3 , files_3)) as fp_3:\n",
    "        soup_3 = BeautifulSoup(fp_3)\n",
    "        tags_3 = soup_3.find_all('s')\n",
    "        all_samples.extend(tags_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3daf1d",
   "metadata": {},
   "source": [
    "Making random selection of 5000 by using random sample taking 5000 sentences from all samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "28f08f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_selection = random.sample(all_samples, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2160fc40",
   "metadata": {},
   "source": [
    "# Implementation of Plain Lesk Algorithm and Most Common Sense for SEMCOR 3.0 DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "af98d01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagtostring(sentence):                   #function converting the corpus tags to string\n",
    "    new_sentence = \"\"\n",
    "    for x in sentence.contents:\n",
    "        if x != '\\n':\n",
    "            new_sentence += x.string\n",
    "            new_sentence += \" \"\n",
    "    return new_sentence  \n",
    "\n",
    "def overlapcontext(synset, sentence):         #Function for getting the overlapcontext \n",
    "    gloss = set(synset.definition().split(\" \"))\n",
    "    sentence = set(sentence.split(\" \"))       #Spliting of input sentence on space\n",
    "    return len( gloss.intersection(sentence) )#getting the intersection between gloss and sentence\n",
    "\n",
    "def lesk(word, sentence):                     #Plain lesk algorithm\n",
    "    bestsense = None\n",
    "    maxoverlap = 0                            #returns the synset with max overlap in terms of meaning\n",
    "    for sense in wn.synsets(word):\n",
    "        overlap = overlapcontext(sense,sentence)\n",
    "        for h in sense.hyponyms():\n",
    "            overlap += overlapcontext( h, sentence ) #getting the maxoverlaps\n",
    "        if overlap > maxoverlap:\n",
    "                maxoverlap = overlap\n",
    "                bestsense = sense             #getting the best sense to return \n",
    "    return bestsense\n",
    "\n",
    "def most_common_sence(word):                  #function to get the most common sense\n",
    "    common_sense = wn.synsets(word)\n",
    "    return None if len(common_sense) == 0 else common_sense[0]\n",
    "\n",
    "def map(pos):                                 #Mapping function used to map noun adj verb adv etc\n",
    "    tag = {'NN':wn.NOUN, 'JJ':wn.ADJ,\n",
    "                  'VB':wn.VERB, 'RB':wn.ADV, 'NNP' :wn.NOUN}\n",
    "    return tag[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6edc725",
   "metadata": {},
   "source": [
    "Running the main algorithm for Lesk Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "4b67155b",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total_checked = 0\n",
    "for i in range(len(random_selection)):\n",
    "    sentences = tagtostring(random_selection[i]) #converting tags to strings\n",
    "    words = sentences.split(\" \")\n",
    "    for w in words:\n",
    "        try:\n",
    "            tag_w = random_selection[i].find(\"wf\",text=w)\n",
    "            if len(wn.synsets(w)) > 1 and tag_w.has_attr(\"lemma\"): #condition if the synsets are greater \n",
    "                                                                   #than one and tags has lemma inside them.\n",
    "                lesk_prediction = lesk(w,sentences)                #applying plain lesk algorithm\n",
    "                tag_ = random_selection[i].find(\"wf\",text=w)\n",
    "                new_tag = '{}.{}.{:02d}'.format(tag_[\"lemma\"], map(tag_[\"pos\"]), int(tag_[\"wnsn\"])) #mapping the pos tag\n",
    "                new_synset = wn.synset(new_tag)\n",
    "                if new_synset is not None and lesk_prediction is not None and  new_synset == lesk_prediction:\n",
    "                    correct +=1\n",
    "                total_checked +=1\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "b01f53d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing results for LESK ALGORITHM\n",
      "Accuracy with comparison of our prediction and ground truth: 35.20931841771422 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / total_checked)* 100\n",
    "print(\"Printing results for LESK ALGORITHM\")\n",
    "print(\"Accuracy with comparison of our prediction and ground truth:\",accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f8bd84",
   "metadata": {},
   "source": [
    "Running the main algorithm for Most Common Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "f5951706",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total_checked = 0\n",
    "for i in range(len(random_selection)):\n",
    "    sentences = tagtostring(random_selection[i])                   #converting tags to strings\n",
    "    words = sentences.split(\" \")\n",
    "    for w in words:\n",
    "        try:\n",
    "            tag_w = random_selection[i].find(\"wf\",text=w)\n",
    "            if len(wn.synsets(w)) > 1 and tag_w.has_attr(\"lemma\"): #condition if the synsets are greater \n",
    "                                                                   #than one and tags has lemma inside them.\n",
    "                lesk_prediction = most_common_sence(w)             #applying most common sense\n",
    "                tag_ = random_selection[i].find(\"wf\",text=w)\n",
    "                new_tag = '{}.{}.{:02d}'.format(tag_[\"lemma\"], map(tag_[\"pos\"]), int(tag_[\"wnsn\"])) #mapping the pos tag\n",
    "                new_synset = wn.synset(new_tag)\n",
    "                if new_synset is not None and lesk_prediction is not None and  new_synset == lesk_prediction:\n",
    "                    correct +=1\n",
    "                total_checked +=1\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "388a020c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing results for MOST COMMON SENSE Algorithm\n",
      "Accuracy with comparison of our prediction and ground truth: 44.98904393956868 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / total_checked)* 100\n",
    "print(\"Printing results for MOST COMMON SENSE Algorithm\")\n",
    "print(\"Accuracy with comparison of our prediction and ground truth:\",accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8010a04",
   "metadata": {},
   "source": [
    "# Paper Implementation for Semcor 3.0 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9915ce",
   "metadata": {},
   "source": [
    "Importing lexeme file from the given embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "3e569c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme = open('lexemes.txt','r')  #getting lexeme file from the embeddings\n",
    "lexeme.readline()\n",
    "lexeme_data = {}\n",
    "temp = []\n",
    "for line in lexeme:\n",
    "    temp = line.strip().split(' ')#Splitting and stripping on space\n",
    "    key = temp[0]\n",
    "    value = temp[1:]\n",
    "    value_ =list(np.float_(value))#converting to float and after float we convert it into a list\n",
    "    lexeme_data[key]=value_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6531041",
   "metadata": {},
   "source": [
    "Doing the sorting as per the paper, by sorting the words in a sentence by there number of synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "eeaf38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting(sentences):                                #sorting function to sort the words according to there number of synsets\n",
    "    no_synsets = []\n",
    "    temp = sentences.strip().split(' ')\n",
    "    for words in temp:\n",
    "        count = len(wn.synsets(words))\n",
    "        no_synsets.append(count)                       #counting the no of synsets for each word and storing in a list\n",
    "        sort = np.argsort(no_synsets)                  #sorting in a acending order\n",
    "        sortings = [x for _,x in sorted(zip(sort,temp))]\n",
    "    return sortings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ea40eb",
   "metadata": {},
   "source": [
    "Importing the Gensim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "3f4ef505",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_embeddings = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d76245",
   "metadata": {},
   "source": [
    "Writting the functions for paper implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "722de833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(lista,listb):                                #Function to find the cosine similarity between two lists\n",
    "    cosine_sim = dot(lista, listb)/(norm(lista)*norm(listb))\n",
    "    return cosine_sim\n",
    "\n",
    "def gloss_embeddings(inputs):                            #function to find the gloss embeddings\n",
    "    addition_vector = np.zeros(shape=(300,))             #addition vector array\n",
    "    vector_array = []\n",
    "    defination = inputs.definition()                     #gets definition of inputs\n",
    "    defination = defination.strip().split(' ')           #splits the definition on space and strip\n",
    "    for words in defination:\n",
    "        try: \n",
    "            gensim = gensim_embeddings.get_vector(words) #apply gensim embeddings on the words in a definition\n",
    "        except Exception: \n",
    "            continue\n",
    "        vector_array.append(gensim)                      #appending the gensim in vector array\n",
    "    for v in vector_array:\n",
    "        addition_vector = addition_vector+v              #adding so that we can do the averaging\n",
    "    averaging = np.array(addition_vector)/len(vector_array) #averaging by addition and length of the vector array\n",
    "    return averaging\n",
    "        \n",
    "def context_embedding(words,sentences):                  #function to get the context embedding\n",
    "    lis = []                                             #empty list array\n",
    "    addition_vector = np.zeros(shape=(300,))             #addition vector array\n",
    "    vector_array = []\n",
    "    sentences = sentences.strip().split(' ')             #Split the input sentences on space\n",
    "    for eachword in sentences:                           #Gets each word in a sentence\n",
    "        if eachword != words:                            #checks if that word is not equal to the input word\n",
    "            lis.append(eachword)\n",
    "    lis = sorting(\" \".join(lis))                         #does the sorting based on the number of synsets\n",
    "    for word in lis:\n",
    "        try: \n",
    "            gensim = gensim_embeddings.get_vector(word)  #Applying gensim embeddings\n",
    "        except Exception: \n",
    "            continue\n",
    "        vector_array.append(gensim)\n",
    "    for v in vector_array:\n",
    "        addition_vector = addition_vector+v              #adding for averaging\n",
    "    averaging =  np.array(addition_vector)/len(vector_array)\n",
    "    return averaging                                     #returning the average\n",
    "            \n",
    "def lesk_paper(word, sentence):                          #Distributional lesk \n",
    "    bestsense = None\n",
    "    max_score = 0                                        #returns the synset with max score\n",
    "    for sense in wn.synsets(word):\n",
    "        string_syn = sense.name().split(\".\")             \n",
    "        lexeme_key = \"{}-wn-2.1-{}-{}\".format(string_syn[0],sense.offset(),string_syn[1])\n",
    "        score = cos_sim(gloss_embeddings(sense),context_embedding(word,sentence)) #calculates the score as per the paper formula\n",
    "        if lexeme_key in lexeme_data.keys():             #as we have now gloss and context and also lexeme key\n",
    "            score = score + cos_sim(lexeme_data[lexeme_key],context_embedding(word,sentence))\n",
    "        if score > max_score:                            #checks if the score is greater\n",
    "                max_score = score\n",
    "                bestsense = sense\n",
    "    return bestsense                                     #returns the best sense based on the score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2817858d",
   "metadata": {},
   "source": [
    "Running the main algorithm for Paper Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "29959d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total_checked = 0\n",
    "for i in range(len(random_selection)):\n",
    "    sentences = tagtostring(random_selection[i])                   #converting tags to strings\n",
    "    words = sentences.split(\" \")\n",
    "    for w in words:\n",
    "        try:\n",
    "            tag_w = random_selection[i].find(\"wf\",text=w)\n",
    "            if len(wn.synsets(w)) > 1 and tag_w.has_attr(\"lemma\"): #condition if the synsets are greater \n",
    "                                                                   #than one and tags has lemma inside them.\n",
    "                lesk_prediction = lesk_paper(w,sentences)          #applying distributional lesk algorithm\n",
    "                tag_ = random_selection[i].find(\"wf\",text=w)\n",
    "                new_tag = '{}.{}.{:02d}'.format(tag_[\"lemma\"], map(tag_[\"pos\"]), int(tag_[\"wnsn\"])) #mapping the pos tag\n",
    "                new_synset = wn.synset(new_tag)\n",
    "                if new_synset is not None and lesk_prediction is not None and  new_synset == lesk_prediction:\n",
    "                    correct +=1\n",
    "                total_checked +=1\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "8079545c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing results for Paper Implemention for SEMCOR 3.0 DATASET\n",
      "Accuracy with comparison of our prediction and ground truth: 45.709971937108367 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / total_checked)* 100\n",
    "print(\"Printing results for Paper Implemention for SEMCOR 3.0 DATASET\")\n",
    "print(\"Accuracy with comparison of our prediction and ground truth:\",accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d30fd56",
   "metadata": {},
   "source": [
    "# Importing Files for Senseval 2 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce4ac81e",
   "metadata": {},
   "source": [
    "Importing Files for Senseval 2 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "5391d53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = []\n",
    "data_archive = 'senseval2.semcor\\\\wordnet3.0'\n",
    "for files in os.listdir(data_archive):\n",
    "    with open(os.path.join(data_archive , files)) as fp:\n",
    "        soup = BeautifulSoup(fp)\n",
    "        tags = soup.find_all('s')\n",
    "        all_samples.extend(tags)\n",
    "random_selection = random.sample(all_samples, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44693e1c",
   "metadata": {},
   "source": [
    "# Implementation of Plain Lesk Algorithm and Most Common Sense for SENSEVAL 2 DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "7f60927a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagtostring(sentence):                   #function converting the corpus tags to string\n",
    "    new_sentence = \"\"\n",
    "    for x in sentence.contents:\n",
    "        if x != '\\n':\n",
    "            new_sentence += x.string\n",
    "            new_sentence += \" \"\n",
    "    return new_sentence  \n",
    "\n",
    "def overlapcontext(synset, sentence):         #Function for getting the overlapcontext \n",
    "    gloss = set(synset.definition().split(\" \"))\n",
    "    sentence = set(sentence.split(\" \"))       #Spliting of input sentence on space\n",
    "    return len( gloss.intersection(sentence) )#getting the intersection between gloss and sentence\n",
    "\n",
    "def lesk(word, sentence):                     #Plain lesk algorithm\n",
    "    bestsense = None\n",
    "    maxoverlap = 0                            #returns the synset with max overlap in terms of meaning\n",
    "    for sense in wn.synsets(word):\n",
    "        overlap = overlapcontext(sense,sentence)\n",
    "        for h in sense.hyponyms():\n",
    "            overlap += overlapcontext( h, sentence ) #getting the maxoverlaps\n",
    "        if overlap > maxoverlap:\n",
    "                maxoverlap = overlap\n",
    "                bestsense = sense             #getting the best sense to return \n",
    "    return bestsense\n",
    "\n",
    "def most_common_sence(word):                  #function to get the most common sense\n",
    "    common_sense = wn.synsets(word)\n",
    "    return None if len(common_sense) == 0 else common_sense[0]\n",
    "\n",
    "def map(pos):                                 #Mapping function used to map noun adj verb adv etc\n",
    "    tag = {'NN':wn.NOUN, 'JJ':wn.ADJ,\n",
    "                  'VB':wn.VERB, 'RB':wn.ADV, 'NNP' :wn.NOUN}\n",
    "    return tag[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41be377",
   "metadata": {},
   "source": [
    "Running the Main Algorithm for Lesk Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "7a8af8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total_checked = 0\n",
    "for i in range(len(random_selection)):\n",
    "    sentences = tagtostring(random_selection[i])                   #converting tags to strings\n",
    "    words = sentences.split(\" \")\n",
    "    for w in words:\n",
    "        try:\n",
    "            tag_w = random_selection[i].find(\"wf\",text=w)\n",
    "            if len(wn.synsets(w)) > 1 and tag_w.has_attr(\"lemma\"): #condition if the synsets are greater \n",
    "                                                                   #than one and tags has lemma inside them.\n",
    "                lesk_prediction = lesk(w,sentences)                #applying plain lesk algorithm\n",
    "                tag_ = random_selection[i].find(\"wf\",text=w)\n",
    "                new_tag = '{}.{}.{:02d}'.format(tag_[\"lemma\"], map(tag_[\"pos\"]), int(tag_[\"wnsn\"])) #mapping the pos tag\n",
    "                new_synset = wn.synset(new_tag)\n",
    "                if new_synset is not None and lesk_prediction is not None and  new_synset == lesk_prediction:\n",
    "                    correct +=1\n",
    "                total_checked +=1\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "933fd367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing for Senseval 2 Dataset\n",
      "Printing results for LESK ALGORITHM\n",
      "Accuracy with comparison of our prediction and ground truth: 35.66433566433567 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / total_checked)* 100\n",
    "print(\"Printing for Senseval 2 Dataset\")\n",
    "print(\"Printing results for LESK ALGORITHM\")\n",
    "print(\"Accuracy with comparison of our prediction and ground truth:\",accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba13b40",
   "metadata": {},
   "source": [
    "Runing the Main Algorithm for Most Common Sense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "bc5822a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total_checked = 0\n",
    "for i in range(len(random_selection)):\n",
    "    sentences = tagtostring(random_selection[i])                   #converting tags to strings\n",
    "    words = sentences.split(\" \")\n",
    "    for w in words:\n",
    "        try:\n",
    "            tag_w = random_selection[i].find(\"wf\",text=w)\n",
    "            if len(wn.synsets(w)) > 1 and tag_w.has_attr(\"lemma\"): #condition if the synsets are greater \n",
    "                                                                   #than one and tags has lemma inside them.\n",
    "                lesk_prediction = most_common_sence(w)             #applying most common sense\n",
    "                tag_ = random_selection[i].find(\"wf\",text=w)\n",
    "                new_tag = '{}.{}.{:02d}'.format(tag_[\"lemma\"], map(tag_[\"pos\"]), int(tag_[\"wnsn\"])) #mapping the pos tag\n",
    "                new_synset = wn.synset(new_tag)\n",
    "                if new_synset is not None and lesk_prediction is not None and  new_synset == lesk_prediction:\n",
    "                    correct +=1\n",
    "                total_checked +=1\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "9251a854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing for Senseval 2 Dataset\n",
      "Printing results for MOST COMMON SENSE Algorithm\n",
      "Accuracy with comparison of our prediction and ground truth: 40.05994005994006 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / total_checked)* 100\n",
    "print(\"Printing for Senseval 2 Dataset\")\n",
    "print(\"Printing results for MOST COMMON SENSE Algorithm\")\n",
    "print(\"Accuracy with comparison of our prediction and ground truth:\",accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaee4ec",
   "metadata": {},
   "source": [
    "# Paper Implementation for Senseval 2 Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd5de6f",
   "metadata": {},
   "source": [
    "Importing lexeme file from the given embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "ff1baa1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme = open('lexemes.txt','r')                      #getting lexeme file from the embeddings\n",
    "lexeme.readline()\n",
    "lexeme_data = {}\n",
    "temp = []\n",
    "for line in lexeme:\n",
    "    temp = line.strip().split(' ')                    #Splitting and stripping on space\n",
    "    key = temp[0]\n",
    "    value = temp[1:]\n",
    "    value_ =list(np.float_(value))                    #converting to float and after float we convert it into a list\n",
    "    lexeme_data[key]=value_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c52aca",
   "metadata": {},
   "source": [
    "Doing the sorting as per the paper, by sorting the words in a sentence by there number of synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "69dae86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting(sentences):                                #sorting function to sort the words according to there number of synsets\n",
    "    no_synsets = []\n",
    "    temp = sentences.strip().split(' ')\n",
    "    for words in temp:\n",
    "        count = len(wn.synsets(words))\n",
    "        no_synsets.append(count)                       #counting the no of synsets for each word and storing in a list\n",
    "        sort = np.argsort(no_synsets)                  #sorting in a acending order\n",
    "        sortings = [x for _,x in sorted(zip(sort,temp))]\n",
    "    return sortings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8af3eb",
   "metadata": {},
   "source": [
    "Importing the Gensim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "f93cd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_embeddings = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783edc7",
   "metadata": {},
   "source": [
    "Writing functions for paper implemention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "73230d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(lista,listb):                                #Function to find the cosine similarity between two lists\n",
    "    cosine_sim = dot(lista, listb)/(norm(lista)*norm(listb))\n",
    "    return cosine_sim\n",
    "\n",
    "def gloss_embeddings(inputs):                            #function to find the gloss embeddings\n",
    "    addition_vector = np.zeros(shape=(300,))             #addition vector array\n",
    "    vector_array = []\n",
    "    defination = inputs.definition()                     #gets definition of inputs\n",
    "    defination = defination.strip().split(' ')           #splits the definition on space and strip\n",
    "    for words in defination:\n",
    "        try: \n",
    "            gensim = gensim_embeddings.get_vector(words) #apply gensim embeddings on the words in a definition\n",
    "        except Exception: \n",
    "            continue\n",
    "        vector_array.append(gensim)                      #appending the gensim in vector array\n",
    "    for v in vector_array:\n",
    "        addition_vector = addition_vector+v              #adding so that we can do the averaging\n",
    "    averaging = np.array(addition_vector)/len(vector_array) #averaging by addition and length of the vector array\n",
    "    return averaging\n",
    "        \n",
    "def context_embedding(words,sentences):                  #function to get the context embedding\n",
    "    lis = []                                             #empty list array\n",
    "    addition_vector = np.zeros(shape=(300,))             #addition vector array\n",
    "    vector_array = []\n",
    "    sentences = sentences.strip().split(' ')             #Split the input sentences on space\n",
    "    for eachword in sentences:                           #Gets each word in a sentence\n",
    "        if eachword != words:                            #checks if that word is not equal to the input word\n",
    "            lis.append(eachword)\n",
    "    lis = sorting(\" \".join(lis))                         #does the sorting based on the number of synsets\n",
    "    for word in lis:\n",
    "        try: \n",
    "            gensim = gensim_embeddings.get_vector(word)  #Applying gensim embeddings\n",
    "        except Exception: \n",
    "            continue\n",
    "        vector_array.append(gensim)\n",
    "    for v in vector_array:\n",
    "        addition_vector = addition_vector+v              #adding for averaging\n",
    "    averaging =  np.array(addition_vector)/len(vector_array)\n",
    "    return averaging                                     #returning the average\n",
    "            \n",
    "def lesk_paper(word, sentence):                          #Distributional lesk \n",
    "    bestsense = None\n",
    "    max_score = 0                                        #returns the synset with max score\n",
    "    for sense in wn.synsets(word):\n",
    "        string_syn = sense.name().split(\".\")             \n",
    "        lexeme_key = \"{}-wn-2.1-{}-{}\".format(string_syn[0],sense.offset(),string_syn[1])\n",
    "        score = cos_sim(gloss_embeddings(sense),context_embedding(word,sentence)) #calculates the score as per the paper formula\n",
    "        if lexeme_key in lexeme_data.keys():             #as we have now gloss and context and also lexeme key\n",
    "            score = score + cos_sim(lexeme_data[lexeme_key],context_embedding(word,sentence))\n",
    "        if score > max_score:                            #checks if the score is greater\n",
    "                max_score = score\n",
    "                bestsense = sense\n",
    "    return bestsense                                     #returns the best sense based on the score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0f25f2",
   "metadata": {},
   "source": [
    "Running the algorithm for Paper implementation for Senseval 2 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "88d8c8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total_checked = 0\n",
    "for i in range(len(random_selection)):\n",
    "    sentences = tagtostring(random_selection[i])                   #converting tags to strings\n",
    "    words = sentences.split(\" \")\n",
    "    for w in words:\n",
    "        try:\n",
    "            tag_w = random_selection[i].find(\"wf\",text=w)\n",
    "            if len(wn.synsets(w)) > 1 and tag_w.has_attr(\"lemma\"): #condition if the synsets are greater \n",
    "                                                                   #than one and tags has lemma inside them.\n",
    "                lesk_prediction = lesk_paper(w,sentences)          #applying distributional lesk algorithm\n",
    "                tag_ = random_selection[i].find(\"wf\",text=w)\n",
    "                new_tag = '{}.{}.{:02d}'.format(tag_[\"lemma\"], map(tag_[\"pos\"]), int(tag_[\"wnsn\"])) #mapping the pos tag\n",
    "                new_synset = wn.synset(new_tag)\n",
    "                if new_synset is not None and lesk_prediction is not None and  new_synset == lesk_prediction:\n",
    "                    correct +=1\n",
    "                total_checked +=1\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "id": "884b2d4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing results for Paper Implemention for SENSVAL 2 DATASET\n",
      "Accuracy with comparison of our prediction and ground truth: 45.01873126873127 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / total_checked)* 100\n",
    "print(\"Printing results for Paper Implemention for SENSVAL 2 DATASET\")\n",
    "print(\"Accuracy with comparison of our prediction and ground truth:\",accuracy,\"%\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ad4603",
   "metadata": {},
   "source": [
    "# Importing files for SENSEVAL 3 DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1024677c",
   "metadata": {},
   "source": [
    "Importing files for Senseval 3 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "264c5bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = []\n",
    "data_archive = 'senseval3.semcor\\\\wordnet3.0'\n",
    "for files in os.listdir(data_archive):\n",
    "    with open(os.path.join(data_archive , files)) as fp:\n",
    "        soup = BeautifulSoup(fp)\n",
    "        tags = soup.find_all('s')\n",
    "        all_samples.extend(tags)\n",
    "random_selection = random.sample(all_samples, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727f059c",
   "metadata": {},
   "source": [
    "# Implementation of Plain Lesk Algorithm and Most Common Sense for SENSEVAL 3 DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "0a71994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagtostring(sentence):                   #function converting the corpus tags to string\n",
    "    new_sentence = \"\"\n",
    "    for x in sentence.contents:\n",
    "        if x != '\\n':\n",
    "            new_sentence += x.string\n",
    "            new_sentence += \" \"\n",
    "    return new_sentence  \n",
    "\n",
    "def overlapcontext(synset, sentence):         #Function for getting the overlapcontext \n",
    "    gloss = set(synset.definition().split(\" \"))\n",
    "    sentence = set(sentence.split(\" \"))       #Spliting of input sentence on space\n",
    "    return len( gloss.intersection(sentence) )#getting the intersection between gloss and sentence\n",
    "\n",
    "def lesk(word, sentence):                     #Plain lesk algorithm\n",
    "    bestsense = None\n",
    "    maxoverlap = 0                            #returns the synset with max overlap in terms of meaning\n",
    "    for sense in wn.synsets(word):\n",
    "        overlap = overlapcontext(sense,sentence)\n",
    "        for h in sense.hyponyms():\n",
    "            overlap += overlapcontext( h, sentence ) #getting the maxoverlaps\n",
    "        if overlap > maxoverlap:\n",
    "                maxoverlap = overlap\n",
    "                bestsense = sense             #getting the best sense to return \n",
    "    return bestsense\n",
    "\n",
    "def most_common_sence(word):                  #function to get the most common sense\n",
    "    common_sense = wn.synsets(word)\n",
    "    return None if len(common_sense) == 0 else common_sense[0]\n",
    "\n",
    "def map(pos):                                 #Mapping function used to map noun adj verb adv etc\n",
    "    tag = {'NN':wn.NOUN, 'JJ':wn.ADJ,\n",
    "                  'VB':wn.VERB, 'RB':wn.ADV, 'NNP' :wn.NOUN}\n",
    "    return tag[pos]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847f29ba",
   "metadata": {},
   "source": [
    "Running main algorithm for Lesk Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "e2319e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total_checked = 0\n",
    "for i in range(len(random_selection)):\n",
    "    sentences = tagtostring(random_selection[i])                  #converting tags to strings\n",
    "    words = sentences.split(\" \")\n",
    "    for w in words:\n",
    "        try:\n",
    "            tag_w = random_selection[i].find(\"wf\",text=w)\n",
    "            if len(wn.synsets(w)) > 1 and tag_w.has_attr(\"lemma\"): #condition if the synsets are greater \n",
    "                                                                   #than one and tags has lemma inside them.\n",
    "                lesk_prediction = lesk(w,sentences)                #applying plain lesk algorithm\n",
    "                tag_ = random_selection[i].find(\"wf\",text=w)\n",
    "                new_tag = '{}.{}.{:02d}'.format(tag_[\"lemma\"], map(tag_[\"pos\"]), int(tag_[\"wnsn\"])) #mapping the pos tag\n",
    "                new_synset = wn.synset(new_tag)\n",
    "                if new_synset is not None and lesk_prediction is not None and  new_synset == lesk_prediction:\n",
    "                    correct +=1\n",
    "                total_checked +=1\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "d3700968",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing for Senseval 3 Dataset\n",
      "Printing results for LESK ALGORITHM\n",
      "Accuracy with comparison of our prediction and ground truth: 36.08490566037736 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / total_checked)* 100\n",
    "print(\"Printing for Senseval 3 Dataset\")\n",
    "print(\"Printing results for LESK ALGORITHM\")\n",
    "print(\"Accuracy with comparison of our prediction and ground truth:\",accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "572cf18a",
   "metadata": {},
   "source": [
    "Running main algorithm for Most Common Sense Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "54b4f862",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total_checked = 0\n",
    "for i in range(len(random_selection)):\n",
    "    sentences = tagtostring(random_selection[i])                   #converting tags to strings\n",
    "    words = sentences.split(\" \")\n",
    "    for w in words:\n",
    "        try:\n",
    "            tag_w = random_selection[i].find(\"wf\",text=w)\n",
    "            if len(wn.synsets(w)) > 1 and tag_w.has_attr(\"lemma\"): #condition if the synsets are greater \n",
    "                                                                   #than one and tags has lemma inside them.\n",
    "                lesk_prediction = most_common_sence(w)             #applying most common sense\n",
    "                tag_ = random_selection[i].find(\"wf\",text=w)\n",
    "                new_tag = '{}.{}.{:02d}'.format(tag_[\"lemma\"], map(tag_[\"pos\"]), int(tag_[\"wnsn\"])) #mapping the pos tag\n",
    "                new_synset = wn.synset(new_tag)\n",
    "                if new_synset is not None and lesk_prediction is not None and  new_synset == lesk_prediction:\n",
    "                    correct +=1\n",
    "                total_checked +=1\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "1cfd4cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing for Senseval 3 Dataset\n",
      "Printing results for MOST COMMON SENSE Algorithm\n",
      "Accuracy with comparison of our prediction and ground truth: 45.04716981132076 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / total_checked)* 100\n",
    "print(\"Printing for Senseval 3 Dataset\")\n",
    "print(\"Printing results for MOST COMMON SENSE Algorithm\")\n",
    "print(\"Accuracy with comparison of our prediction and ground truth:\",accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277dcd6f",
   "metadata": {},
   "source": [
    "# Paper Implementation for Senseval 3 Dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e9a14b",
   "metadata": {},
   "source": [
    "Importing the Lexeme File from the given embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "e5f8e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme = open('lexemes.txt','r')                      #getting lexeme file from the embeddings\n",
    "lexeme.readline()\n",
    "lexeme_data = {}\n",
    "temp = []\n",
    "for line in lexeme:\n",
    "    temp = line.strip().split(' ')                    #Splitting and stripping on space\n",
    "    key = temp[0]\n",
    "    value = temp[1:]\n",
    "    value_ =list(np.float_(value))                    #converting to float and after float we convert it into a list\n",
    "    lexeme_data[key]=value_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf351ac3",
   "metadata": {},
   "source": [
    "Doing the sorting as per the paper, by sorting the words in a sentence by there number of synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "d9df1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting(sentences):                                #sorting function to sort the words according to there number of synsets\n",
    "    no_synsets = []\n",
    "    temp = sentences.strip().split(' ')\n",
    "    for words in temp:\n",
    "        count = len(wn.synsets(words))\n",
    "        no_synsets.append(count)                       #counting the no of synsets for each word and storing in a list\n",
    "        sort = np.argsort(no_synsets)                  #sorting in a acending order\n",
    "        sortings = [x for _,x in sorted(zip(sort,temp))]\n",
    "    return sortings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e49a6c3",
   "metadata": {},
   "source": [
    "Importing the Gensim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "65099c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_embeddings = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e5734a",
   "metadata": {},
   "source": [
    "Writing Functions for paper implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "57b5a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(lista,listb):                                #Function to find the cosine similarity between two lists\n",
    "    cosine_sim = dot(lista, listb)/(norm(lista)*norm(listb))\n",
    "    return cosine_sim\n",
    "\n",
    "def gloss_embeddings(inputs):                            #function to find the gloss embeddings\n",
    "    addition_vector = np.zeros(shape=(300,))             #addition vector array\n",
    "    vector_array = []\n",
    "    defination = inputs.definition()                     #gets definition of inputs\n",
    "    defination = defination.strip().split(' ')           #splits the definition on space and strip\n",
    "    for words in defination:\n",
    "        try: \n",
    "            gensim = gensim_embeddings.get_vector(words) #apply gensim embeddings on the words in a definition\n",
    "        except Exception: \n",
    "            continue\n",
    "        vector_array.append(gensim)                      #appending the gensim in vector array\n",
    "    for v in vector_array:\n",
    "        addition_vector = addition_vector+v              #adding so that we can do the averaging\n",
    "    averaging = np.array(addition_vector)/len(vector_array) #averaging by addition and length of the vector array\n",
    "    return averaging\n",
    "        \n",
    "def context_embedding(words,sentences):                  #function to get the context embedding\n",
    "    lis = []                                             #empty list array\n",
    "    addition_vector = np.zeros(shape=(300,))             #addition vector array\n",
    "    vector_array = []\n",
    "    sentences = sentences.strip().split(' ')             #Split the input sentences on space\n",
    "    for eachword in sentences:                           #Gets each word in a sentence\n",
    "        if eachword != words:                            #checks if that word is not equal to the input word\n",
    "            lis.append(eachword)\n",
    "    lis = sorting(\" \".join(lis))                         #does the sorting based on the number of synsets\n",
    "    for word in lis:\n",
    "        try: \n",
    "            gensim = gensim_embeddings.get_vector(word)  #Applying gensim embeddings\n",
    "        except Exception: \n",
    "            continue\n",
    "        vector_array.append(gensim)\n",
    "    for v in vector_array:\n",
    "        addition_vector = addition_vector+v              #adding for averaging\n",
    "    averaging =  np.array(addition_vector)/len(vector_array)\n",
    "    return averaging                                     #returning the average\n",
    "            \n",
    "def lesk_paper(word, sentence):                          #Distributional lesk \n",
    "    bestsense = None\n",
    "    max_score = 0                                        #returns the synset with max score\n",
    "    for sense in wn.synsets(word):\n",
    "        string_syn = sense.name().split(\".\")             \n",
    "        lexeme_key = \"{}-wn-2.1-{}-{}\".format(string_syn[0],sense.offset(),string_syn[1])\n",
    "        score = cos_sim(gloss_embeddings(sense),context_embedding(word,sentence)) #calculates the score as per the paper formula\n",
    "        if lexeme_key in lexeme_data.keys():             #as we have now gloss and context and also lexeme key\n",
    "            score = score + cos_sim(lexeme_data[lexeme_key],context_embedding(word,sentence))\n",
    "        if score > max_score:                            #checks if the score is greater\n",
    "                max_score = score\n",
    "                bestsense = sense\n",
    "    return bestsense                                     #returns the best sense based on the score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbd59c6",
   "metadata": {},
   "source": [
    "Running the main algorithm for paper implementation for Senseval 3 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "02c53c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total_checked = 0\n",
    "for i in range(len(random_selection)):\n",
    "    sentences = tagtostring(random_selection[i])                   #converting tags to strings\n",
    "    words = sentences.split(\" \")\n",
    "    for w in words:\n",
    "        try:\n",
    "            tag_w = random_selection[i].find(\"wf\",text=w)\n",
    "            if len(wn.synsets(w)) > 1 and tag_w.has_attr(\"lemma\"): #condition if the synsets are greater \n",
    "                                                                   #than one and tags has lemma inside them.\n",
    "                lesk_prediction = lesk_paper(w,sentences)          #applying distributional lesk algorithm\n",
    "                tag_ = random_selection[i].find(\"wf\",text=w)\n",
    "                new_tag = '{}.{}.{:02d}'.format(tag_[\"lemma\"], map(tag_[\"pos\"]), int(tag_[\"wnsn\"])) #mapping the pos tag\n",
    "                new_synset = wn.synset(new_tag)\n",
    "                if new_synset is not None and lesk_prediction is not None and  new_synset == lesk_prediction:\n",
    "                    correct +=1\n",
    "                total_checked +=1\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "197ba697",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing results for Paper Implemention for SENSVAL 3 DATASET\n",
      "Accuracy with comparison of our prediction and ground truth: 46.450943396226417 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / total_checked)* 100\n",
    "print(\"Printing results for Paper Implemention for SENSVAL 3 DATASET\")\n",
    "print(\"Accuracy with comparison of our prediction and ground truth: \",accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fa1328",
   "metadata": {},
   "source": [
    "# Applying Extensions on the Paper Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ced2ec",
   "metadata": {},
   "source": [
    "###### 1 : For SEMCOR 3.0 DATASET\n",
    "###### Experiment with removing stopwords and punctuation from the dictionary glosses,sensedescriptionsandcontextsintheoccurrencesofthewordsbefore measuring the distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3fb0a9",
   "metadata": {},
   "source": [
    "Importing Libraries for stopwords and Punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "4499e0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize  \n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bf62d24",
   "metadata": {},
   "source": [
    "Importing the Semcor 3.0 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "9cc7c3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = []\n",
    "data_archive = 'semcor3.0\\\\brown1\\\\tagfiles'\n",
    "for files in os.listdir(data_archive):\n",
    "    with open(os.path.join(data_archive , files)) as fp:\n",
    "        soup = BeautifulSoup(fp)\n",
    "        tags = soup.find_all('s')\n",
    "        all_samples.extend(tags)\n",
    "data_archive_2 = 'semcor3.0\\\\brown2\\\\tagfiles'\n",
    "for files_2 in os.listdir(data_archive_2):\n",
    "    with open(os.path.join(data_archive_2 , files_2)) as fp_2:\n",
    "        soup_2 = BeautifulSoup(fp_2)\n",
    "        tags_2 = soup_2.find_all('s')\n",
    "        all_samples.extend(tags_2)\n",
    "data_archive_3 = 'semcor3.0\\\\brownv\\\\tagfiles'\n",
    "for files_3 in os.listdir(data_archive_3):\n",
    "    with open(os.path.join(data_archive_3 , files_3)) as fp_3:\n",
    "        soup_3 = BeautifulSoup(fp_3)\n",
    "        tags_3 = soup_3.find_all('s')\n",
    "        all_samples.extend(tags_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b1a58",
   "metadata": {},
   "source": [
    "Randomly selecting 5000 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "6bd8333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_selection =random.sample(all_samples, 5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7a164b",
   "metadata": {},
   "source": [
    "Importing the lexeme embedding file from the given embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "90d124dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme = open('lexemes.txt','r')  #getting lexeme file from the embeddings\n",
    "lexeme.readline()\n",
    "lexeme_data = {}\n",
    "temp = []\n",
    "for line in lexeme:\n",
    "    temp = line.strip().split(' ')#Splitting and stripping on space\n",
    "    key = temp[0]\n",
    "    value = temp[1:]\n",
    "    value_ =list(np.float_(value))#converting to float and after float we convert it into a list\n",
    "    lexeme_data[key]=value_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e90708",
   "metadata": {},
   "source": [
    "Sorting the words according to there no of synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "id": "0f958195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting(sentences):                                #sorting function to sort the words according to there number of synsets\n",
    "    no_synsets = []\n",
    "    temp = sentences.strip().split(' ')\n",
    "    for words in temp:\n",
    "        count = len(wn.synsets(words))\n",
    "        no_synsets.append(count)                       #counting the no of synsets for each word and storing in a list\n",
    "        sort = np.argsort(no_synsets)                  #sorting in a acending order\n",
    "        sortings = [x for _,x in sorted(zip(sort,temp))]\n",
    "    return sortings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb901bd4",
   "metadata": {},
   "source": [
    "Loading the Gensim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "d76b4cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_embeddings = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3683550",
   "metadata": {},
   "source": [
    "Writing functions for the main paper implementation, in this we included stopwords and punctuation removal too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "bf1ffbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(lista,listb):                                #Function to find the cosine similarity between two lists\n",
    "    cosine_sim = dot(lista, listb)/(norm(lista)*norm(listb))\n",
    "    return cosine_sim\n",
    "\n",
    "def gloss_embeddings(inputs):                            #function to find the gloss embeddings\n",
    "    addition_vector = np.zeros(shape=(300,))             #addition vector array\n",
    "    vector_array = []\n",
    "    defination = inputs.definition()                     #gets definition of inputs\n",
    "    stop_words = set(stopwords.words('english'))         #Importing english stopwards\n",
    "    defination = defination.translate(str.maketrans('', '', string.punctuation))   # Removing punctuations\n",
    "    word_tokens = word_tokenize(defination)              #tokenizing the definations\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] #checking for stopwords and removing them\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)                  #filtering out the stopwords\n",
    "    for words in filtered_sentence:\n",
    "        try: \n",
    "            gensim = gensim_embeddings.get_vector(words) #apply gensim embeddings on the words in a definition\n",
    "        except Exception: \n",
    "            continue\n",
    "        vector_array.append(gensim)                      #appending the gensim in vector array\n",
    "    for v in vector_array:\n",
    "        addition_vector = addition_vector+v              #adding so that we can do the averaging\n",
    "    averaging = np.array(addition_vector)/len(vector_array) #averaging by addition and length of the vector array\n",
    "    return averaging\n",
    "        \n",
    "def context_embedding(words,sentences):                  #function to get the context embedding\n",
    "    lis = []                                             #Empty list array to store\n",
    "    addition_vector = np.zeros(shape=(300,))             #addition vector array\n",
    "    vector_array = []\n",
    "    sentences = sentences.translate(str.maketrans('', '', string.punctuation)) #Removing punctuations\n",
    "    stop_words = set(stopwords.words('english'))         #Importing english stopwards\n",
    "    word_tokens = word_tokenize(sentences)               #tokenizing the definations\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] #checking for stopwords and removing them\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)                  #filtering out the stopwords\n",
    "    for eachword in filtered_sentence:                           \n",
    "        if eachword != words:                            \n",
    "            lis.append(eachword)\n",
    "    lis = sorting(\" \".join(lis))                         \n",
    "    for word in lis:\n",
    "        try: \n",
    "            gensim = gensim_embeddings.get_vector(word)  #Applying gensim embeddings\n",
    "        except Exception: \n",
    "            continue\n",
    "        vector_array.append(gensim)\n",
    "    for v in vector_array:\n",
    "        addition_vector = addition_vector+v              #adding for averaging\n",
    "    averaging =  np.array(addition_vector)/len(vector_array)\n",
    "    return averaging                                     #returning the average\n",
    "            \n",
    "def lesk_paper(word, sentence):                          #Distributional lesk \n",
    "    bestsense = None\n",
    "    max_score = 0                                        #returns the synset with max score\n",
    "    for sense in wn.synsets(word):\n",
    "        string_syn = sense.name().split(\".\")             \n",
    "        lexeme_key = \"{}-wn-2.1-{}-{}\".format(string_syn[0],sense.offset(),string_syn[1])\n",
    "        score = cos_sim(gloss_embeddings(sense),context_embedding(word,sentence)) #calculates the score as per the paper formula\n",
    "        if lexeme_key in lexeme_data.keys():             #as we have now gloss and context and also lexeme key\n",
    "            score = score + cos_sim(lexeme_data[lexeme_key],context_embedding(word,sentence))\n",
    "        if score > max_score:                            #checks if the score is greater\n",
    "                max_score = score\n",
    "                bestsense = sense\n",
    "    return bestsense                                     #returns the best sense based on the score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "891480cd",
   "metadata": {},
   "source": [
    "Running the main paper implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8db918d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total_checked = 0\n",
    "for i in range(len(random_selection)):\n",
    "    sentences = tagtostring(random_selection[i])                   #converting tags to strings\n",
    "    words = sentences.split(\" \")\n",
    "    for w in words:\n",
    "        try:\n",
    "            tag_w = random_selection[i].find(\"wf\",text=w)\n",
    "            if len(wn.synsets(w)) > 1 and tag_w.has_attr(\"lemma\"): #condition if the synsets are greater \n",
    "                                                                   #than one and tags has lemma inside them.\n",
    "                lesk_prediction = lesk_paper(w,sentences)          #applying distributional lesk algorithm\n",
    "                tag_ = random_selection[i].find(\"wf\",text=w)\n",
    "                new_tag = '{}.{}.{:02d}'.format(tag_[\"lemma\"], map(tag_[\"pos\"]), int(tag_[\"wnsn\"])) #mapping the pos tag\n",
    "                new_synset = wn.synset(new_tag)\n",
    "                if new_synset is not None and lesk_prediction is not None and  new_synset == lesk_prediction:\n",
    "                    correct +=1\n",
    "                total_checked +=1\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "a3fedea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "EXTENSION FOR STOP WORDS \n",
      "Dataset: Semcor 3.0\n",
      "*************************\n",
      "Printing results for Paper Implemention for Semcor 3.0 DATASET\n",
      "Accuracy with comparison of our prediction and ground truth: 46.35587028716546 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / total_checked)* 100\n",
    "print(\"*************************\")\n",
    "print(\"EXTENSION FOR STOP WORDS \")\n",
    "print(\"Dataset: Semcor 3.0\")\n",
    "print(\"*************************\")\n",
    "print(\"Printing results for Paper Implemention for Semcor 3.0 DATASET\")\n",
    "print(\"Accuracy with comparison of our prediction and ground truth:\",accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc557f6",
   "metadata": {},
   "source": [
    "###### 1 : For SENSVAL 2\n",
    "###### Experiment with removing stopwords and punctuation from the dictionary glosses,sensedescriptionsandcontextsintheoccurrencesofthewordsbefore measuring the distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f949964f",
   "metadata": {},
   "source": [
    "Importing The senseval 2 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "bc82a680",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = []\n",
    "data_archive = 'senseval2.semcor\\\\wordnet3.0'\n",
    "for files in os.listdir(data_archive):\n",
    "    with open(os.path.join(data_archive , files)) as fp:\n",
    "        soup = BeautifulSoup(fp)\n",
    "        tags = soup.find_all('s')\n",
    "        all_samples.extend(tags)\n",
    "random_selection = random.sample(all_samples, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4bb9c4",
   "metadata": {},
   "source": [
    "Reading the lexeme file from the given embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "e761d1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme = open('lexemes.txt','r')                      #getting lexeme file from the embeddings\n",
    "lexeme.readline()\n",
    "lexeme_data = {}\n",
    "temp = []\n",
    "for line in lexeme:\n",
    "    temp = line.strip().split(' ')                    #Splitting and stripping on space\n",
    "    key = temp[0]\n",
    "    value = temp[1:]\n",
    "    value_ =list(np.float_(value))                    #converting to float and after float we convert it into a list\n",
    "    lexeme_data[key]=value_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b054d7e2",
   "metadata": {},
   "source": [
    "Sorting the words according to there number of synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "2f6201ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting(sentences):                                #sorting function to sort the words according to there number of synsets\n",
    "    no_synsets = []\n",
    "    temp = sentences.strip().split(' ')\n",
    "    for words in temp:\n",
    "        count = len(wn.synsets(words))\n",
    "        no_synsets.append(count)                       #counting the no of synsets for each word and storing in a list\n",
    "        sort = np.argsort(no_synsets)                  #sorting in a acending order\n",
    "        sortings = [x for _,x in sorted(zip(sort,temp))]\n",
    "    return sortings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78e2c32",
   "metadata": {},
   "source": [
    "Loading the Gensim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "5bc031aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_embeddings = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb4927e",
   "metadata": {},
   "source": [
    "Writing functions for the main paper implementation, in this we included stopwords and punctuation removal too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "8e6249c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(lista,listb):                                #Function to find the cosine similarity between two lists\n",
    "    cosine_sim = dot(lista, listb)/(norm(lista)*norm(listb))\n",
    "    return cosine_sim\n",
    "\n",
    "def gloss_embeddings(inputs):                            #function to find the gloss embeddings\n",
    "    addition_vector = np.zeros(shape=(300,))             #addition vector array\n",
    "    vector_array = []\n",
    "    defination = inputs.definition()                     #gets definition of inputs\n",
    "    stop_words = set(stopwords.words('english'))         #Importing english stopwards\n",
    "    defination = defination.translate(str.maketrans('', '', string.punctuation))   # Removing punctuations\n",
    "    word_tokens = word_tokenize(defination)              #tokenizing the definations\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] #checking for stopwords and removing them\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)                  #filtering out the stopwords\n",
    "    for words in filtered_sentence:\n",
    "        try: \n",
    "            gensim = gensim_embeddings.get_vector(words) #apply gensim embeddings on the words in a definition\n",
    "        except Exception: \n",
    "            continue\n",
    "        vector_array.append(gensim)                      #appending the gensim in vector array\n",
    "    for v in vector_array:\n",
    "        addition_vector = addition_vector+v              #adding so that we can do the averaging\n",
    "    averaging = np.array(addition_vector)/len(vector_array) #averaging by addition and length of the vector array\n",
    "    return averaging\n",
    "        \n",
    "def context_embedding(words,sentences):                  #function to get the context embedding\n",
    "    lis = []                                             #Empty list array to store\n",
    "    addition_vector = np.zeros(shape=(300,))             #addition vector array\n",
    "    vector_array = []\n",
    "    sentences = sentences.translate(str.maketrans('', '', string.punctuation)) #Removing punctuations\n",
    "    stop_words = set(stopwords.words('english'))         #Importing english stopwards\n",
    "    word_tokens = word_tokenize(sentences)               #tokenizing the definations\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] #checking for stopwords and removing them\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)                  #filtering out the stopwords\n",
    "    for eachword in filtered_sentence:                           \n",
    "        if eachword != words:                            \n",
    "            lis.append(eachword)\n",
    "    lis = sorting(\" \".join(lis))                         \n",
    "    for word in lis:\n",
    "        try: \n",
    "            gensim = gensim_embeddings.get_vector(word)  #Applying gensim embeddings\n",
    "        except Exception: \n",
    "            continue\n",
    "        vector_array.append(gensim)\n",
    "    for v in vector_array:\n",
    "        addition_vector = addition_vector+v              #adding for averaging\n",
    "    averaging =  np.array(addition_vector)/len(vector_array)\n",
    "    return averaging                                     #returning the average\n",
    "            \n",
    "def lesk_paper(word, sentence):                          #Distributional lesk \n",
    "    bestsense = None\n",
    "    max_score = 0                                        #returns the synset with max score\n",
    "    for sense in wn.synsets(word):\n",
    "        string_syn = sense.name().split(\".\")             \n",
    "        lexeme_key = \"{}-wn-2.1-{}-{}\".format(string_syn[0],sense.offset(),string_syn[1])\n",
    "        score = cos_sim(gloss_embeddings(sense),context_embedding(word,sentence)) #calculates the score as per the paper formula\n",
    "        if lexeme_key in lexeme_data.keys():             #as we have now gloss and context and also lexeme key\n",
    "            score = score + cos_sim(lexeme_data[lexeme_key],context_embedding(word,sentence))\n",
    "        if score > max_score:                            #checks if the score is greater\n",
    "                max_score = score\n",
    "                bestsense = sense\n",
    "    return bestsense                                     #returns the best sense based on the score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1486120a",
   "metadata": {},
   "source": [
    "Running the main paper implementation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "904b8d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total_checked = 0\n",
    "for i in range(len(random_selection)):\n",
    "    sentences = tagtostring(random_selection[i])                   #converting tags to strings\n",
    "    words = sentences.split(\" \")\n",
    "    for w in words:\n",
    "        try:\n",
    "            tag_w = random_selection[i].find(\"wf\",text=w)\n",
    "            if len(wn.synsets(w)) > 1 and tag_w.has_attr(\"lemma\"): #condition if the synsets are greater \n",
    "                                                                   #than one and tags has lemma inside them.\n",
    "                lesk_prediction = lesk_paper(w,sentences)          #applying distributional lesk algorithm\n",
    "                tag_ = random_selection[i].find(\"wf\",text=w)\n",
    "                new_tag = '{}.{}.{:02d}'.format(tag_[\"lemma\"], map(tag_[\"pos\"]), int(tag_[\"wnsn\"])) #mapping the pos tag\n",
    "                new_synset = wn.synset(new_tag)\n",
    "                if new_synset is not None and lesk_prediction is not None and  new_synset == lesk_prediction:\n",
    "                    correct +=1\n",
    "                total_checked +=1\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "9840d4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "EXTENSION FOR STOP WORDS \n",
      "Dataset: Senseval 2.0\n",
      "*************************\n",
      "Printing results for Paper Implemention for Senseval 3.0 DATASET\n",
      "Accuracy with comparison of our prediction and ground truth: 47.4212698412698 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / total_checked)* 100\n",
    "print(\"*************************\")\n",
    "print(\"EXTENSION FOR STOP WORDS \")\n",
    "print(\"Dataset: Senseval 2.0\")\n",
    "print(\"*************************\")\n",
    "print(\"Printing results for Paper Implemention for Senseval 3.0 DATASET\")\n",
    "print(\"Accuracy with comparison of our prediction and ground truth:\",accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c295ea4",
   "metadata": {},
   "source": [
    "###### 1 : For SENSVAL 3\n",
    "###### Experiment with removing stopwords and punctuation from the dictionary glosses,sensedescriptionsandcontextsintheoccurrencesofthewordsbefore measuring the distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23253848",
   "metadata": {},
   "source": [
    "Importing the Senseval 3 Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "1732dde5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples = []\n",
    "data_archive = 'senseval3.semcor\\\\wordnet3.0'\n",
    "for files in os.listdir(data_archive):\n",
    "    with open(os.path.join(data_archive , files)) as fp:\n",
    "        soup = BeautifulSoup(fp)\n",
    "        tags = soup.find_all('s')\n",
    "        all_samples.extend(tags)\n",
    "random_selection = random.sample(all_samples, 250)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813a205",
   "metadata": {},
   "source": [
    "Reading the Lexeme file from the given embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "id": "3782ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lexeme = open('lexemes.txt','r')                      #getting lexeme file from the embeddings\n",
    "lexeme.readline()\n",
    "lexeme_data = {}\n",
    "temp = []\n",
    "for line in lexeme:\n",
    "    temp = line.strip().split(' ')                    #Splitting and stripping on space\n",
    "    key = temp[0]\n",
    "    value = temp[1:]\n",
    "    value_ =list(np.float_(value))                    #converting to float and after float we convert it into a list\n",
    "    lexeme_data[key]=value_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db03c96",
   "metadata": {},
   "source": [
    "Sorting the words according to there number of synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "id": "cf9fd65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sorting(sentences):                                #sorting function to sort the words according to there number of synsets\n",
    "    no_synsets = []\n",
    "    temp = sentences.strip().split(' ')\n",
    "    for words in temp:\n",
    "        count = len(wn.synsets(words))\n",
    "        no_synsets.append(count)                       #counting the no of synsets for each word and storing in a list\n",
    "        sort = np.argsort(no_synsets)                  #sorting in a acending order\n",
    "        sortings = [x for _,x in sorted(zip(sort,temp))]\n",
    "    return sortings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e142e85",
   "metadata": {},
   "source": [
    "Loading the Gensim Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "id": "c3e36c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "gensim_embeddings = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b29de73",
   "metadata": {},
   "source": [
    "Writing functions for the main paper implementation, in this we included stopwords and punctuation removal too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "815fbd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(lista,listb):                                #Function to find the cosine similarity between two lists\n",
    "    cosine_sim = dot(lista, listb)/(norm(lista)*norm(listb))\n",
    "    return cosine_sim\n",
    "\n",
    "def gloss_embeddings(inputs):                            #function to find the gloss embeddings\n",
    "    addition_vector = np.zeros(shape=(300,))             #addition vector array\n",
    "    vector_array = []\n",
    "    defination = inputs.definition()                     #gets definition of inputs\n",
    "    stop_words = set(stopwords.words('english'))         #Importing english stopwards\n",
    "    defination = defination.translate(str.maketrans('', '', string.punctuation))   # Removing punctuations\n",
    "    word_tokens = word_tokenize(defination)              #tokenizing the definations\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] #checking for stopwords and removing them\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)                  #filtering out the stopwords\n",
    "    for words in filtered_sentence:\n",
    "        try: \n",
    "            gensim = gensim_embeddings.get_vector(words) #apply gensim embeddings on the words in a definition\n",
    "        except Exception: \n",
    "            continue\n",
    "        vector_array.append(gensim)                      #appending the gensim in vector array\n",
    "    for v in vector_array:\n",
    "        addition_vector = addition_vector+v              #adding so that we can do the averaging\n",
    "    averaging = np.array(addition_vector)/len(vector_array) #averaging by addition and length of the vector array\n",
    "    return averaging\n",
    "        \n",
    "def context_embedding(words,sentences):                  #function to get the context embedding\n",
    "    lis = []                                             #Empty list array to store\n",
    "    addition_vector = np.zeros(shape=(300,))             #addition vector array\n",
    "    vector_array = []\n",
    "    sentences = sentences.translate(str.maketrans('', '', string.punctuation)) #Removing punctuations\n",
    "    stop_words = set(stopwords.words('english'))         #Importing english stopwards\n",
    "    word_tokens = word_tokenize(sentences)               #tokenizing the definations\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words] #checking for stopwords and removing them\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)                  #filtering out the stopwords\n",
    "    for eachword in filtered_sentence:                           \n",
    "        if eachword != words:                            \n",
    "            lis.append(eachword)\n",
    "    lis = sorting(\" \".join(lis))                         \n",
    "    for word in lis:\n",
    "        try: \n",
    "            gensim = gensim_embeddings.get_vector(word)  #Applying gensim embeddings\n",
    "        except Exception: \n",
    "            continue\n",
    "        vector_array.append(gensim)\n",
    "    for v in vector_array:\n",
    "        addition_vector = addition_vector+v              #adding for averaging\n",
    "    averaging =  np.array(addition_vector)/len(vector_array)\n",
    "    return averaging                                     #returning the average\n",
    "            \n",
    "def lesk_paper(word, sentence):                          #Distributional lesk \n",
    "    bestsense = None\n",
    "    max_score = 0                                        #returns the synset with max score\n",
    "    for sense in wn.synsets(word):\n",
    "        string_syn = sense.name().split(\".\")             \n",
    "        lexeme_key = \"{}-wn-2.1-{}-{}\".format(string_syn[0],sense.offset(),string_syn[1])\n",
    "        score = cos_sim(gloss_embeddings(sense),context_embedding(word,sentence)) #calculates the score as per the paper formula\n",
    "        if lexeme_key in lexeme_data.keys():             #as we have now gloss and context and also lexeme key\n",
    "            score = score + cos_sim(lexeme_data[lexeme_key],context_embedding(word,sentence))\n",
    "        if score > max_score:                            #checks if the score is greater\n",
    "                max_score = score\n",
    "                bestsense = sense\n",
    "    return bestsense                                     #returns the best sense based on the score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73665643",
   "metadata": {},
   "source": [
    "Running the main paper implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "id": "0762bc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total_checked = 0\n",
    "for i in range(len(random_selection)):\n",
    "    sentences = tagtostring(random_selection[i])                   #converting tags to strings\n",
    "    words = sentences.split(\" \")\n",
    "    for w in words:\n",
    "        try:\n",
    "            tag_w = random_selection[i].find(\"wf\",text=w)\n",
    "            if len(wn.synsets(w)) > 1 and tag_w.has_attr(\"lemma\"): #condition if the synsets are greater \n",
    "                                                                   #than one and tags has lemma inside them.\n",
    "                lesk_prediction = lesk_paper(w,sentences)          #applying distributional lesk algorithm\n",
    "                tag_ = random_selection[i].find(\"wf\",text=w)\n",
    "                new_tag = '{}.{}.{:02d}'.format(tag_[\"lemma\"], map(tag_[\"pos\"]), int(tag_[\"wnsn\"])) #mapping the pos tag\n",
    "                new_synset = wn.synset(new_tag)\n",
    "                if new_synset is not None and lesk_prediction is not None and  new_synset == lesk_prediction:\n",
    "                    correct +=1\n",
    "                total_checked +=1\n",
    "        except Exception:\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "d953919a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************\n",
      "EXTENSION FOR STOP WORDS \n",
      "Dataset: Senseval 3.0\n",
      "*************************\n",
      "Printing results for Paper Implemention for Senseval 3.0 DATASET\n",
      "Accuracy with comparison of our prediction and ground truth: 47.004982290436838 %\n"
     ]
    }
   ],
   "source": [
    "accuracy = (correct / total_checked)* 100\n",
    "print(\"*************************\")\n",
    "print(\"EXTENSION FOR STOP WORDS \")\n",
    "print(\"Dataset: Senseval 3.0\")\n",
    "print(\"*************************\")\n",
    "print(\"Printing results for Paper Implemention for Senseval 3.0 DATASET\")\n",
    "print(\"Accuracy with comparison of our prediction and ground truth:\",accuracy,\"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
